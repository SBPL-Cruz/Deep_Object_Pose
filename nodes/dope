#!/usr/bin/env python

# Copyright (c) 2018 NVIDIA Corporation. All rights reserved.
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode

"""
This file starts a ROS node to run DOPE, 
listening to an image topic and publishing poses.
"""

from __future__ import print_function

import cv2
import message_filters
import numpy as np
import resource_retriever
import rospy
import tf.transformations
from PIL import Image
from PIL import ImageDraw
from cv_bridge import CvBridge
from dope.inference.cuboid import Cuboid3d
from dope.inference.cuboid_pnp_solver import CuboidPNPSolver
from dope.inference.detector import ModelData, ObjectDetector
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import PointCloud2, CameraInfo, Image as ImageSensor_msg
from std_msgs.msg import String
from vision_msgs.msg import Detection3D, Detection3DArray, ObjectHypothesisWithPose
from visualization_msgs.msg import Marker, MarkerArray
import pcl
import ros_numpy
from plyfile import PlyData, PlyElement
import time
import rospkg
from sklearn.metrics import pairwise_distances_chunked, pairwise_distances_argmin_min

class Draw(object):
    """Drawing helper class to visualize the neural network output"""

    def __init__(self, im):
        """
        :param im: The image to draw in.
        """
        self.draw = ImageDraw.Draw(im)

    def draw_line(self, point1, point2, line_color, line_width=2):
        """Draws line on image"""
        if point1 is not None and point2 is not None:
            self.draw.line([point1, point2], fill=line_color, width=line_width)

    def draw_dot(self, point, point_color, point_radius):
        """Draws dot (filled circle) on image"""
        if point is not None:
            xy = [
                point[0] - point_radius,
                point[1] - point_radius,
                point[0] + point_radius,
                point[1] + point_radius
            ]
            self.draw.ellipse(xy,
                              fill=point_color,
                              outline=point_color
                              )

    def draw_cube(self, points, color=(255, 0, 0)):
        """
        Draws cube with a thick solid line across
        the front top edge and an X on the top face.
        """

        # draw front
        self.draw_line(points[0], points[1], color)
        self.draw_line(points[1], points[2], color)
        self.draw_line(points[3], points[2], color)
        self.draw_line(points[3], points[0], color)

        # draw back
        self.draw_line(points[4], points[5], color)
        self.draw_line(points[6], points[5], color)
        self.draw_line(points[6], points[7], color)
        self.draw_line(points[4], points[7], color)

        # draw sides
        self.draw_line(points[0], points[4], color)
        self.draw_line(points[7], points[3], color)
        self.draw_line(points[5], points[1], color)
        self.draw_line(points[2], points[6], color)

        # draw dots
        self.draw_dot(points[0], point_color=color, point_radius=4)
        self.draw_dot(points[1], point_color=color, point_radius=4)

        # draw x on the top
        self.draw_line(points[0], points[5], color)
        self.draw_line(points[1], points[4], color)


class DopeNode(object):
    """ROS node that listens to image topic, runs DOPE, and publishes DOPE results"""
    def __init__(self):
        self.pubs = {}
        self.models = {}
        self.pnp_solvers = {}
        self.pub_dimension = {}
        self.draw_colors = {}
        self.dimensions = {}
        self.class_ids = {}
        self.model_transforms = {}
        self.meshes = {}
        self.mesh_scales = {}
        self.mesh_clouds = {}
        self.cv_bridge = CvBridge()

        self.input_is_rectified = rospy.get_param('~input_is_rectified', True)
        self.downscale_height = rospy.get_param('~downscale_height', 500)
        self.use_icp = rospy.get_param('~use_icp', False)
        self.use_cloud = rospy.get_param('~use_cloud', False)

        self.config_detect = lambda: None
        self.config_detect.mask_edges = 1
        self.config_detect.mask_faces = 1
        self.config_detect.vertex = 1
        self.config_detect.threshold = 0.5
        self.config_detect.softmax = 1000
        self.config_detect.thresh_angle = rospy.get_param('~thresh_angle', 0.5)
        self.config_detect.thresh_map = rospy.get_param('~thresh_map', 0.01)
        self.config_detect.sigma = rospy.get_param('~sigma', 3)
        self.config_detect.thresh_points = rospy.get_param("~thresh_points", 0.1)
        self.tf_listener = tf.TransformListener()
        self.world_frame =  rospy.get_param("~world_frame", "/base_footprint")
        self.camera_frame =  rospy.get_param("~camera_frame", "/camera_depth_optical_frame")
        self.zmin = rospy.get_param('~zmin', 0.5)
        self.zmax = rospy.get_param('~zmax', 1.0)
        self.xmin = rospy.get_param('~xmin', 0.0)
        self.xmax = rospy.get_param('~xmax', 1.0)
        self.downsampling_leaf_size = rospy.get_param('~downsampling_leaf_size', 0.02)
        self.track_pose_accuracy = rospy.get_param('~track_pose_accuracy', False)
        self.min_pose_error = {}

        self.filter_y = rospy.get_param('~filter_y', False)
        self.backup_icp = rospy.get_param('~backup_icp', False)
        self.use_dope = rospy.get_param('~use_dope', False)

        # For each object to detect, load network model, create PNP solver, and start ROS publishers
        for model, weights_url in rospy.get_param('~weights').iteritems():
            self.models[model] = \
                ModelData(
                    model,
                    resource_retriever.get_filename(weights_url, use_protocol=False)
                )
            if self.use_dope:
                self.models[model].load_net_model()
            self.min_pose_error[model] = np.inf

            try:
                M = np.array(rospy.get_param('~model_transforms')[model], dtype='float64')
                self.model_transforms[model] = tf.transformations.quaternion_from_matrix(M)
            except KeyError:
                self.model_transforms[model] = np.array([0.0, 0.0, 0.0, 1.0], dtype='float64')

            try:
                self.meshes[model] = rospy.get_param('~meshes')[model]
            except KeyError:
                pass
            
            

            try:
                self.mesh_scales[model] = rospy.get_param('~mesh_scales')[model]
            except KeyError:
                self.mesh_scales[model] = 1.0

            try:
                cloud = PlyData.read(rospy.get_param('~meshes_ply')[model]).elements[0].data
                cloud = np.transpose(np.vstack((cloud['x'], cloud['y'], cloud['z'])))
                
                cloud_pose = pcl.PointCloud()
                cloud_pose.from_array(cloud)
                sor = cloud_pose.make_voxel_grid_filter()
                sor.set_leaf_size(self.downsampling_leaf_size, self.downsampling_leaf_size, self.downsampling_leaf_size)
                cloud_pose = sor.filter()

                self.mesh_clouds[model] = np.asarray(cloud_pose)
                # Points x 3 for dim of below
                rospy.logwarn("Loaded mesh cloud for : {} with size : {}, scaling : {}".format(model, cloud.shape[0], self.mesh_scales[model]))
                # scale_transform = tf.transformations.scale_matrix(self.mesh_scales[model])
                # cloud = np.hstack((cloud, np.ones((cloud.shape[0], 1))))
                # cloud = np.matmul(scale_transform, np.transpose(cloud))
                # self.mesh_clouds[model] = np.transpose(cloud)[:, :3]
            except KeyError:
                pass

            self.draw_colors[model] = tuple(rospy.get_param("~draw_colors")[model])
            self.dimensions[model] = tuple(rospy.get_param("~dimensions")[model])
            self.class_ids[model] = rospy.get_param("~class_ids")[model]

            self.pnp_solvers[model] = \
                CuboidPNPSolver(
                    model,
                    cuboid3d=Cuboid3d(rospy.get_param('~dimensions')[model])
                )
            self.pubs[model] = \
                rospy.Publisher(
                    '{}/pose_{}'.format(rospy.get_param('~topic_publishing'), model),
                    PoseStamped,
                    queue_size=10
                )
            self.pub_dimension[model] = \
                rospy.Publisher(
                    '{}/dimension_{}'.format(rospy.get_param('~topic_publishing'), model),
                    String,
                    queue_size=10
                )

        # Start ROS publishers
        self.pub_rgb_dope_points = \
            rospy.Publisher(
                rospy.get_param('~topic_publishing') + "/rgb_points",
                ImageSensor_msg,
                queue_size=10
            )
        self.pub_detections = \
            rospy.Publisher(
                '~detected_objects',
                Detection3DArray,
                queue_size=10
            )
        self.pub_markers = \
            rospy.Publisher(
                '~markers',
                MarkerArray,
                queue_size=10
            )
        self.pub_filtered_cloud = \
            rospy.Publisher(
                rospy.get_param('~topic_publishing') + "/filtered_cloud",
                PointCloud2,
                queue_size=10
            )

        self.pub_pose_cloud = \
            rospy.Publisher(
                rospy.get_param('~topic_publishing') + "/pose_cloud",
                PointCloud2,
                queue_size=10
            )

        # Start ROS subscriber
        image_sub = message_filters.Subscriber(
            rospy.get_param('~topic_camera'),
            ImageSensor_msg
        )
        depth_image_sub = message_filters.Subscriber(
            rospy.get_param('~topic_depth_camera'),
            ImageSensor_msg
        )
        info_sub = message_filters.Subscriber(
            rospy.get_param('~topic_camera_info'),
            CameraInfo
        )
        cloud_sub = message_filters.Subscriber(
            rospy.get_param('~topic_camera_cloud'),
            PointCloud2
        )
        rospack = rospkg.RosPack()
        dope_path = rospack.get_path('dope')
        # self.pose_file = open(dope_path + "/dope_icp_predictions_1.txt", "w")
        self.pose_file = open(rospy.get_param("~pose_file", "pose_file.txt"), "w")
        # self.pose_file = open(dope_path + "/dope_predictions_2.txt", "w")
        # if self.use_icp:
        #     cloud_sub.registerCallback(self.cloud_callback)

        # ts = message_filters.TimeSynchronizer([image_sub, depth_image_sub], 10)
        # ts.registerCallback(self.image_depth_callback)

        if self.use_icp or self.use_cloud:
            self.camera_info = None
            self.camera_pose_matrix = None

            self.get_camera_pose()

            # ts = message_filters.ApproximateTimeSynchronizer([image_sub, info_sub, cloud_sub], 1, 10, 0.1)
            self.rgb_image_cache = message_filters.Cache(image_sub, 100, allow_headerless=False)
            cloud_sub.registerCallback(self.cloud_callback)
            info_sub.registerCallback(self.camera_info_callback)
        else:
            
            ts = message_filters.ApproximateTimeSynchronizer([image_sub, info_sub], 10, 0.1, allow_headerless=True)
            ts.registerCallback(self.image_callback)

        self.pose_taken = False
        self.pose_close_file = open(dope_path + "/pose_close_percep.txt", "a+")

        print("Running DOPE...(Listening to camera topic: '{}', info topic : '{}', cloud topic : '{}')".format(
                    rospy.get_param('~topic_camera'), rospy.get_param('~topic_camera_info'), rospy.get_param('~topic_camera_cloud')))
        print("Ctrl-C to stop")

    # def image_depth_callback(self, rgb, depth):
    #     print("test")

    def get_camera_pose(self):
        try:
            rospy.logdebug("Getting transform between : {} and {}".format(self.world_frame, self.camera_frame))
            trans, quat = self.tf_listener.lookupTransform(self.world_frame, self.camera_frame, rospy.Time(0))
            # Camera pose in world frame
            R = tf.transformations.quaternion_matrix(quat)
            T = tf.transformations.translation_matrix(trans)
            self.camera_pose_matrix = tf.transformations.concatenate_matrices(T, R)
            rospy.logwarn("Got camera pose : {}".format(self.camera_pose_matrix))
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
            rospy.logerr("Couldnt get camera pose")

    def camera_info_callback(self, camera_info):
        self.camera_info = camera_info

    def xyzrgb_array_to_pointcloud2(self, points, colors, stamp=None, frame_id=None, seq=None):
        '''
        Create a sensor_msgs.PointCloud2 from an array
        of points.
        '''
        from sensor_msgs.msg import PointCloud2, PointField

        msg = PointCloud2()
        # assert(points.shape == colors.shape)
        colors = np.zeros(points.shape)
        buf = []

        if stamp:
            msg.header.stamp = stamp
        if frame_id:
            msg.header.frame_id = frame_id
        if seq:
            msg.header.seq = seq
        if len(points.shape) == 3:
            msg.height = points.shape[1]
            msg.width = points.shape[0]
        else:
            N = len(points)
            xyzrgb = np.array(np.hstack([points, colors]), dtype=np.float32)
            msg.height = 1
            msg.width = N

        msg.fields = [
            PointField('x', 0, PointField.FLOAT32, 1),
            PointField('y', 4, PointField.FLOAT32, 1),
            PointField('z', 8, PointField.FLOAT32, 1),
            PointField('r', 12, PointField.FLOAT32, 1),
            PointField('g', 16, PointField.FLOAT32, 1),
            PointField('b', 20, PointField.FLOAT32, 1)
        ]
        msg.is_bigendian = False
        msg.point_step = 24
        msg.row_step = msg.point_step * N
        msg.is_dense = True
        msg.data = xyzrgb.tostring()

        return msg

    def cloud_callback(self, cloud_msg):
        # rospy.logdebug("Cloud received, frame : {}".format(cloud_msg.header.frame_id))
        rospy.logdebug("Cloud received delay : {}".format(rospy.Time.now().to_sec() - cloud_msg.header.stamp.to_sec()))
        latest_image_msg = self.rgb_image_cache.getElemBeforeTime(cloud_msg.header.stamp)
        if self.camera_pose_matrix is None:
            self.get_camera_pose()
        if self.camera_pose_matrix is not None and latest_image_msg is not None and self.camera_info is not None:
            # print(cloud_msg.header.stamp.to_sec())
            # print(latest_image_msg.header.stamp.to_sec())
            self.image_callback(latest_image_msg, self.camera_info, cloud_msg)

    def process_cloud(self, cloud_msg, publish_filtered=False):
        '''
            1. Transform to robot frame
            2. Filter in z and x
            3. Downsample
            4. Publish and return filtered cloud
        '''
        # rospy.logdebug("Cloud received, frame : {}".format(cloud_msg.header.frame_id))

        pc = ros_numpy.numpify(cloud_msg)
        # pc_l = [np.asarray(x) for x in pc]
        # print(pc_l)
        # pc = np.asarray(pc_l)
        # height = pc.shape[0]
        # width = pc.shape[1]
        # np_points = np.zeros((height * width, 3), dtype=np.float32)
        np_points = np.zeros((pc.shape[0], 3), dtype=np.float32)
        np_points[:, 0] = np.resize(pc['x'], pc.shape[0])
        np_points[:, 1] = np.resize(pc['y'], pc.shape[0])
        np_points[:, 2] = np.resize(pc['z'], pc.shape[0])

        # np_points_appened = np.hstack((np_points, np.ones((np_points.shape[0], 1))))
        # np_points_transformed = np.matmul(total_transform, np.transpose(np_points_appened))
        # np_points_transformed = np.transpose(np_points_transformed)[:,:3]
        # np_points_transformed = self.transform_cloud(np_points, mat=self.camera_pose_matrix)
        # pcl_cloud = pcl.PointCloud(np_points_transformed)

        # passthrough = pcl_cloud.make_passthrough_filter()
        # passthrough.set_filter_field_name("z")
        # passthrough.set_filter_limits(self.zmin, self.zmax)
        # cloud_filtered = passthrough.filter()

        # passthrough = cloud_filtered.make_passthrough_filter()
        # passthrough.set_filter_field_name("x")
        # passthrough.set_filter_limits(self.xmin, self.xmax)
        # cloud_filtered = passthrough.filter()

        # passthrough = cloud_filtered.make_passthrough_filter()
        # passthrough.set_filter_field_name("y")
        # passthrough.set_filter_limits(0, 1.7)
        # cloud_filtered = passthrough.filter()

        # fil = cloud_filtered.make_statistical_outlier_filter()
        # fil.set_mean_k(100)
        # fil.set_std_dev_mul_thresh(0.1)
        # cloud_filtered = fil.filter()

        # sor = cloud_filtered.make_voxel_grid_filter()
        # sor.set_leaf_size(self.downsampling_leaf_size, self.downsampling_leaf_size, self.downsampling_leaf_size)
        # cloud_filtered = sor.filter()

        # fil = cloud_filtered.make_statistical_outlier_filter()
        # fil.set_mean_k(100)
        # fil.set_std_dev_mul_thresh(0.08)
        # cloud_filtered = fil.filter()

        # cloud_filtered_array = np.asarray(cloud_filtered)  
        cloud_filtered_array = np_points  
        cloud_filtered = pcl.PointCloud(np_points)
        mean = np.mean(cloud_filtered_array, axis=0)
        mean[2] = self.zmin
        pose_estimate = {}
        mean = mean * 100
        pose_estimate['location'] = mean.tolist()
        # pose_estimate['quaternion'] = [0.02218474, -0.81609224, -0.57647473,  0.03432471]
        pose_estimate['quaternion'] = [-1,0,0,1]
        
        rospy.logdebug ("Num points after downsample and filter : {}".format(cloud_filtered_array.shape[0]))
        
        if publish_filtered:
            cloud_color = np.zeros(cloud_filtered_array.shape[0])
            ros_msg = self.xyzrgb_array_to_pointcloud2(
                cloud_filtered_array, cloud_color, rospy.Time.now(), self.world_frame
            )
            self.pub_filtered_cloud.publish(ros_msg)
        rospy.logdebug("Done cloud processing")
        
        return cloud_filtered, pose_estimate

    def transform_cloud(self, cloud_in, trans=None, quat=None, mat=None):
        '''
            Tranform point cloud np array
        '''
        if trans is not None and quat is not None:
            R = tf.transformations.quaternion_matrix(quat)
            T = tf.transformations.translation_matrix(trans)
            total_transform = tf.transformations.concatenate_matrices(T, R)
        elif mat is not None:
            total_transform = mat
        cloud_in = np.hstack((cloud_in, np.ones((cloud_in.shape[0], 1))))
        cloud_out = np.matmul(total_transform, np.transpose(cloud_in))
        cloud_out = np.transpose(cloud_out)[:,:3]
        cloud_out = np.array(cloud_out, dtype=np.float32)
        return cloud_out

    def compare_clouds(self, scene_cloud, transformed_pose_cloud):
        # print(scene_cloud.shape)
        # print(transformed_pose_cloud.shape)
        pairwise_distances = pairwise_distances_argmin_min(
                    scene_cloud, transformed_pose_cloud, metric='euclidean')
        mean_dist_add_s = np.mean(pairwise_distances[1])
        return mean_dist_add_s


    def image_callback(self, image_msg, camera_info, cloud_in=None):
        """Image callback"""

        start_time = time.time()
        if self.use_icp or self.use_cloud:
            cloud_scene, pose_estimate = self.process_cloud(cloud_in, publish_filtered=False)
            cloud_array = np.asarray(cloud_scene)
            if (cloud_array.shape[0] < 10):
                return
        img = self.cv_bridge.imgmsg_to_cv2(image_msg, "rgb8")
        # cv2.imwrite('img.png', cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # for debugging

        # Update camera matrix and distortion coefficients
        
        if self.input_is_rectified:
            P = np.matrix(camera_info.P, dtype='float64')
            P.resize((3, 4))
            camera_matrix = P[:, :3]
            dist_coeffs = np.zeros((4, 1))
        else:
            camera_matrix = np.matrix(camera_info.K, dtype='float64')
            camera_matrix.resize((3, 3))
            dist_coeffs = np.matrix(camera_info.D, dtype='float64')
            dist_coeffs.resize((len(camera_info.D), 1))

        # Downscale image if necessary
        height, width, _ = img.shape
        scaling_factor = float(self.downscale_height) / height
        if scaling_factor < 1.0:
            camera_matrix[:2] *= scaling_factor
            img = cv2.resize(img, (int(scaling_factor * width), int(scaling_factor * height)))

        for m in self.models:
            self.pnp_solvers[m].set_camera_intrinsic_matrix(camera_matrix)
            self.pnp_solvers[m].set_dist_coeffs(dist_coeffs)

        # Copy and draw image
        img_copy = img.copy()
        im = Image.fromarray(img_copy)
        draw = Draw(im)

        detection_array = Detection3DArray()
        detection_array.header = image_msg.header

        # TODO : Move to config
        # filter_y = True
        # backup_icp = False
        # use_dope = True

        for m in self.models:
            # Detect object
            results = []
            if self.use_dope:
                results = ObjectDetector.detect_object_in_image(
                    self.models[m].net,
                    self.pnp_solvers[m],
                    img,
                    self.config_detect
                )
            
            if (len(results)) == 0 and self.backup_icp == True:
                result = {}
                result["score"] = 0.0
                result["name"] = "sugar"
                result["location"] = pose_estimate["location"]
                result["quaternion"] = np.array(pose_estimate["quaternion"])
                results.append(result)
                pose_frame = self.world_frame
                do_icp = True
            else:
                do_icp = True
                pose_frame = image_msg.header.frame_id

            # Publish pose and overlay cube on image
            for i_r, result in enumerate(results):
                if result["location"] is None:
                    continue

                loc = result["location"]
                ori = result["quaternion"]
                CONVERT_SCALE_CM_TO_METERS = 100
                
                loc_scale = np.array([loc[0] / CONVERT_SCALE_CM_TO_METERS, loc[1] / CONVERT_SCALE_CM_TO_METERS, loc[2] / CONVERT_SCALE_CM_TO_METERS])
                R = tf.transformations.quaternion_matrix(ori)
                T = tf.transformations.translation_matrix(loc_scale)

                # Pose is in camera frame, so need to apply camera as well as pose transformations
                if pose_frame == self.world_frame:
                    total_transform = tf.transformations.concatenate_matrices(T, R)
                else:
                    total_transform = tf.transformations.concatenate_matrices(self.camera_pose_matrix, T, R)
                
                if self.use_icp and do_icp:
                    # rospy.logwarn("Doing ICP for result : {}, {}".format(i_r, result["name"]))

                    cloud_filtered_array = self.transform_cloud(self.mesh_clouds[result["name"]], mat=total_transform)
                    
                    cloud_pose = pcl.PointCloud()
                    cloud_pose.from_array(cloud_filtered_array)

                    # Do ICP and get new transform
                    icp = cloud_pose.make_IterativeClosestPoint()
                    converged, transf, estimate, fitness = icp.icp(cloud_pose, cloud_scene)
                    # rospy.logdebug('has converged:' + str(converged) + ' score: ' + str(fitness))
                    # rospy.logdebug(str(transf))
                    total_transform_icp = tf.transformations.concatenate_matrices(transf, total_transform)
                    # best_fitness_score = np.inf
                    # for yaw in np.linspace(0, 3.14, 1):
                    #     rospy.logwarn("Sampling yaw : {}".format(yaw))
                    #     R_sample = tf.transformations.euler_matrix(0, yaw, 0)
                    #     # print(R_sample)
                    #     sample_transform = tf.transformations.concatenate_matrices(total_transform, R_sample)
                    #     # Transform model cloud with prediction that can be used for ICP
                    #     cloud_filtered_array = self.transform_cloud(self.mesh_clouds[result["name"]], mat=sample_transform)
                        
                    #     cloud_pose = pcl.PointCloud()
                    #     cloud_pose.from_array(cloud_filtered_array)

                    #     # Do ICP and get new transform
                    #     icp = cloud_pose.make_IterativeClosestPoint()
                    #     converged, transf, estimate, fitness = icp.icp(cloud_pose, cloud_scene)
                    #     rospy.logdebug('has converged:' + str(converged) + ' score: ' + str(fitness))
                    #     rospy.logdebug(str(transf))

                    #     # _, yaw_icp, _ = tf.transformations.euler_from_matrix(transf)
                    #     # trans_icp = tf.transformations.translation_matrix(tf.transformations.translation_from_matrix(transf))
                    #     # # print(yaw_icp)
                    #     # transf = tf.transformations.concatenate_matrices(trans_icp, tf.transformations.euler_matrix(0, 0, 0))

                    #     icp_transform = tf.transformations.concatenate_matrices(transf, sample_transform)
                    #     # icp_trans = tf.transformations.translation_from_matrix(icp_transform)
                    #     cloud_filtered_array = self.transform_cloud(self.mesh_clouds[result["name"]], mat=icp_transform)
                    #     # cloud_color = np.zeros(cloud_filtered_array.shape[0])
                    #     # pose_cloud_msg = self.xyzrgb_array_to_pointcloud2(
                    #     #     cloud_filtered_array, cloud_color, rospy.Time.now(), self.world_frame
                    #     # )
                    #     # self.pub_pose_cloud.publish(pose_cloud_msg)
                    #     # rospy.sleep(5)
                    #     fitness = self.compare_clouds(np.asarray(cloud_scene), cloud_filtered_array)
                    #     if fitness < best_fitness_score:
                    #         best_fitness_score = fitness
                    #         total_transform_icp = icp_transform

                    # rospy.logdebug(str(total_transform_icp))
                else:
                    total_transform_icp = total_transform

                # Use the final pose to get loc and ori lists which can be used later
                loc = tf.transformations.translation_from_matrix(total_transform_icp)
                CONVERT_SCALE_CM_TO_METERS = 1
                ori = tf.transformations.quaternion_from_matrix(total_transform_icp)
                pose_frame = self.world_frame

                # Use the final pose to create model point cloud and get accuracy
                cloud_filtered_array = self.transform_cloud(self.mesh_clouds[result["name"]], mat=total_transform_icp)
                pose_error = self.compare_clouds(np.asarray(cloud_scene), cloud_filtered_array)
                # rospy.logwarn("Average pose distance - ADD-S (in m) : {}, min pose error so far : {}".format(pose_error, self.min_pose_error[result["name"]]))
                
                pose_list = loc.tolist() + ori.tolist()
                pose_list += [pose_error]
                self.pose_file.write(' '.join(map(str, pose_list)))
                self.pose_file.write("\n")

                # Only publish if new pose is better than last one and if tracking pose accuracy is switched on
                

                cloud_color = np.zeros(cloud_filtered_array.shape[0])
                pose_cloud_msg = self.xyzrgb_array_to_pointcloud2(
                    cloud_filtered_array, cloud_color, rospy.Time.now(), self.world_frame
                )

                # transform orientation
                transformed_ori = tf.transformations.quaternion_multiply(ori, self.model_transforms[m])

                # rotate bbox dimensions if necessary
                # (this only works properly if model_transform is in 90 degree angles)
                dims = rotate_vector(vector=self.dimensions[m], quaternion=self.model_transforms[m])
                dims = np.absolute(dims)
                dims = tuple(dims)

                pose_msg = PoseStamped()
                pose_msg.header = cloud_in.header
                pose_msg.header.frame_id = pose_frame
                # pose_msg.header.stamp = rospy.Time.now()
                pose_msg.pose.position.x = loc[0] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.position.y = loc[1] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.position.z = loc[2] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.orientation.x = transformed_ori[0]
                pose_msg.pose.orientation.y = transformed_ori[1]
                pose_msg.pose.orientation.z = transformed_ori[2]
                pose_msg.pose.orientation.w = transformed_ori[3]


                # Publish
                if (pose_msg.pose.position.y < 1.49 and pose_msg.pose.position.y > 1.38 and self.filter_y == True) or self.filter_y == False:
                    self.pose_taken = False
                
                if (pose_msg.pose.position.y <= 1.05 and pose_msg.pose.position.y >= 0.95 and self.filter_y == True) or self.filter_y == False:
                    rospy.logdebug(str(total_transform_icp))
                    
                    if self.pose_taken == False:
                        time_offset = (rospy.Time.now().to_sec() - cloud_in.header.stamp.to_sec()) + (time.time() - start_time) + 0.7
                        offset_distance = time_offset * 0.2
                        pose_actual = pose_msg.pose
                        pose_actual.position.y -= offset_distance
                        rospy.logwarn("Received pose : {}, Time offset : {}, Actual pose : {}".format(pose_msg.pose.position, time_offset, pose_actual.position))
                        # pose_actual_euler = tf.transformations.euler_from_quaternion(transformed_ori, 'rzyx')
                        # rospy.logwarn("Actual pose yaw : {}".format(pose_actual_euler))
                        self.pose_taken = True
                        # yaw = pose_actual_euler[2]
                        # yaw += 2 * np.pi 
                        # self.pose_close_file.write('x : {}, y : {}, yaw : {} '.format(pose_actual.position.x, pose_actual.position.y, yaw))
                        self.pose_close_file.write("{} {} {} {} {} {}".format(
                            pose_actual.position.x, pose_actual.position.y,
                            pose_actual.orientation.x, pose_actual.orientation.y, pose_actual.orientation.z, pose_actual.orientation.w
                        ))
                        self.pose_close_file.write("\n")

                    # if pose_msg.pose.position.y < 1.49 and pose_msg.pose.position.y > 1.47:
                    #     self.first_pose_y = pose_msg.pose.position.y
                    #     self.first_pose_timestamp = pose_msg.header.stamp.to_sec()
                    # else:
                    #     distance_from_pose = self.first_pose_y - pose_msg.pose.position.y
                    #     distance_from_time = (pose_msg.header.stamp.to_sec() - self.first_pose_timestamp)  * 0.2
                    #     distance_difference = distance_from_time - distance_from_pose
                    #     rospy.logdebug("First pose Y : {}".format(self.first_pose_y))
                    #     rospy.logdebug("Distance from time : {}, distance from pose : {}, difference : {}".format(distance_from_time, distance_from_pose, distance_difference))

                    if self.track_pose_accuracy:
                        if pose_error < self.min_pose_error[result["name"]]:
                            self.min_pose_error[result["name"]] = pose_error
                        else:
                            continue
                    self.pubs[m].publish(pose_msg)
                    self.pub_dimension[m].publish(str(dims))
                    if self.use_icp or self.use_cloud:
                        self.pub_pose_cloud.publish(pose_cloud_msg)

                    # Add to Detection3DArray
                    detection = Detection3D()
                    hypothesis = ObjectHypothesisWithPose()
                    hypothesis.id = self.class_ids[result["name"]]
                    # hypothesis.score = result["score"]
                    hypothesis.score = pose_error
                    hypothesis.pose.pose = pose_msg.pose
                    detection.results.append(hypothesis)
                    detection.bbox.center = pose_msg.pose
                    detection.bbox.size.x = dims[0] / CONVERT_SCALE_CM_TO_METERS
                    detection.bbox.size.y = dims[1] / CONVERT_SCALE_CM_TO_METERS
                    detection.bbox.size.z = dims[2] / CONVERT_SCALE_CM_TO_METERS
                    detection_array.detections.append(detection)

                    # Draw the cube
                    if 'projected_points' in result:
                        if None not in result['projected_points']:
                            points2d = []
                            for pair in result['projected_points']:
                                points2d.append(tuple(pair))
                            draw.draw_cube(points2d, self.draw_colors[m])

        # Publish the image with results overlaid
        # if (pose_msg.pose.position.y < 1.38 and pose_msg.pose.position.y > 0.9 and self.filter_y == True) or self.filter_y == False:
        self.pub_rgb_dope_points.publish(
            CvBridge().cv2_to_imgmsg(
                np.array(im)[..., ::-1],
                "bgr8"
            )
        )
        self.pub_detections.publish(detection_array)
        self.publish_markers(detection_array)
            


        print("--- Estimation took %s seconds ---" % (time.time() - start_time))


    def publish_markers(self, detection_array):
        # Delete all existing markers
        markers = MarkerArray()
        marker = Marker()
        marker.action = Marker.DELETEALL
        markers.markers.append(marker)
        self.pub_markers.publish(markers)

        # Object markers
        class_id_to_name = {class_id: name for name, class_id in self.class_ids.iteritems()}
        markers = MarkerArray()
        for i, det in enumerate(detection_array.detections):
            name = class_id_to_name[det.results[0].id]
            color = self.draw_colors[name]

            # cube marker
            # marker = Marker()
            # marker.header = detection_array.header
            # marker.action = Marker.ADD
            # marker.pose = det.bbox.center
            # marker.color.r = color[0] / 255.0
            # marker.color.g = color[1] / 255.0
            # marker.color.b = color[2] / 255.0
            # marker.color.a = 0.4
            # marker.ns = "bboxes"
            # marker.id = i
            # marker.type = Marker.CUBE
            # marker.scale = det.bbox.size
            # markers.markers.append(marker)

            # text marker
            marker = Marker()
            marker.header = detection_array.header
            marker.action = Marker.ADD
            marker.pose = det.bbox.center
            marker.pose.position.z += 0.20
            marker.color.r = color[0] / 255.0
            marker.color.g = color[1] / 255.0
            marker.color.b = color[2] / 255.0
            marker.color.a = 1.0
            marker.id = i
            marker.ns = "texts"
            marker.type = Marker.TEXT_VIEW_FACING
            marker.scale.x = 0.05
            marker.scale.y = 0.05
            marker.scale.z = 0.05
            marker.text = '{} (add:{:.4f}, y:{:.2f})'.format(name, det.results[0].score, marker.pose.position.y)
            markers.markers.append(marker)

            # mesh marker
            # try:
            #     marker = Marker()
            #     marker.header = detection_array.header
            #     marker.action = Marker.ADD
            #     marker.pose = det.bbox.center
            #     marker.color.r = color[0] / 255.0
            #     marker.color.g = color[1] / 255.0
            #     marker.color.b = color[2] / 255.0
            #     marker.color.a = 0.7
            #     marker.ns = "meshes"
            #     marker.id = i
            #     marker.type = Marker.MESH_RESOURCE
            #     marker.scale.x = self.mesh_scales[name]
            #     marker.scale.y = self.mesh_scales[name]
            #     marker.scale.z = self.mesh_scales[name]
            #     marker.mesh_resource = self.meshes[name]
            #     markers.markers.append(marker)
            # except KeyError:
            #     # user didn't specify self.meshes[name], so don't publish marker
            #     pass

        self.pub_markers.publish(markers)


def rotate_vector(vector, quaternion):
    q_conj = tf.transformations.quaternion_conjugate(quaternion)
    vector = np.array(vector, dtype='float64')
    vector = np.append(vector, [0.0])
    vector = tf.transformations.quaternion_multiply(q_conj, vector)
    vector = tf.transformations.quaternion_multiply(vector, quaternion)
    return vector[:3]


def main():
    """Main routine to run DOPE"""

    # Initialize ROS node
    rospy.init_node('dope', log_level=rospy.DEBUG)
    DopeNode()

    try:
        rospy.spin()
    except rospy.ROSInterruptException:
        pass


if __name__ == "__main__":
    main()
